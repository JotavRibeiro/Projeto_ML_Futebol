{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from utils import plot_metrics\n",
    "import json\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "# Machine learning and optimization\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from optuna.storages import JournalStorage, JournalFileStorage\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    StandardScaler,\n",
    "    RobustScaler,\n",
    "    QuantileTransformer\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import WOEEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "import mlflow\n",
    "\n",
    "# Rich console output\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize console\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set general directory of the project\n",
    "def general_directory():\n",
    "    \"\"\"\n",
    "    Move up one level in the directory hierarchy.\n",
    "    \"\"\"\n",
    "    # Get the current directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Get the parent directory\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "    # Change to the parent directory\n",
    "    os.chdir(parent_dir)\n",
    "\n",
    "# Set actual directory of the project\n",
    "def actual_directory():\n",
    "    \"\"\"\n",
    "    Move down to the 'experiments' subdirectory from the current directory.\n",
    "    \"\"\"\n",
    "    # Get the current directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Name of the subdirectory to move into\n",
    "    sub_dir_name = \"experiments\"  # Replace with the name of your subdirectory\n",
    "\n",
    "    # Construct the full path to the subdirectory\n",
    "    sub_dir_path = os.path.join(current_dir, sub_dir_name)\n",
    "\n",
    "    # Change to the subdirectory\n",
    "    os.chdir(sub_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_directory()\n",
    "\n",
    "df = pd.read_parquet(os.getcwd() + '\\\\data\\\\silver\\\\BRA_baseline.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clubes = pd.DataFrame()\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "for i in list(df['Date'].unique()):\n",
    "\n",
    "  df_mandante  = (df[df['Date'] == i]\n",
    "                  .groupby(['Home','Season'])\n",
    "                  .agg({'Date': 'first',\n",
    "                        'HG':'first',\n",
    "                        'AG':'first',\n",
    "                        'AvgH':'first',\n",
    "                        'AvgA':'first'})).reset_index()\n",
    "\n",
    "  df_visitante  = (df[df['Date'] == i]\n",
    "                   .groupby(['Away','Season'])\n",
    "                   .agg({'Date':'first',\n",
    "                         'AG':'first',\n",
    "                         'HG':'first',\n",
    "                         'AvgA':'first',\n",
    "                         'AvgH':'first'})).reset_index()\n",
    "\n",
    "  df_mandante['mando'] = 1\n",
    "  df_mandante['saldo_de_gols'] = df_mandante['HG'] - df_mandante['AG']\n",
    "  df_mandante['flag_vitoria'] = np.where(df_mandante['HG'] > df_mandante['AG'], 1, 0)\n",
    "  df_mandante['flag_favorito'] = np.where(df_mandante['AvgH'] < df_mandante['AvgA'], 1, 0)\n",
    "\n",
    "  df_visitante['mando'] = 0\n",
    "  df_visitante['saldo_de_gols'] = df_visitante['AG'] - df_visitante['HG']\n",
    "  df_visitante['flag_vitoria'] = np.where(df_visitante['AG'] > df_visitante['HG'], 1, 0)\n",
    "  df_visitante['flag_favorito'] = np.where(df_visitante['AvgA'] < df_visitante['AvgH'], 1, 0)\n",
    "\n",
    "  df_mandante.columns = [\n",
    "                        'clube',\n",
    "                        'temporada',\n",
    "                        'data',\n",
    "                        'gols_marcados',\n",
    "                        'gols_sofridos',\n",
    "                        'odd_vitoria',\n",
    "                        'odd_derrota',\n",
    "                        'mando',\n",
    "                        'saldo_de_gols',\n",
    "                        'flag_vitoria',\n",
    "                        'flag_favorito',\n",
    "                        ]\n",
    "  \n",
    "  df_visitante.columns = [\n",
    "                         'clube',\n",
    "                         'temporada',\n",
    "                         'data',\n",
    "                         'gols_marcados',\n",
    "                         'gols_sofridos',\n",
    "                         'odd_vitoria',\n",
    "                         'odd_derrota',\n",
    "                         'mando',\n",
    "                         'saldo_de_gols',\n",
    "                         'flag_vitoria',\n",
    "                         'flag_favorito',\n",
    "                         ]\n",
    "\n",
    "  concatened_data = pd.concat([df_mandante, df_visitante])\n",
    "\n",
    "  df_clubes = pd.concat([df_clubes,concatened_data])\n",
    "\n",
    "df_clubes.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trendline(valores, order=1):\n",
    "    x = np.arange(1, len(valores) + 1)  \n",
    "    coeffs = np.polyfit(x, valores, order)\n",
    "    slope = coeffs[-2]\n",
    "    return float(slope)\n",
    "\n",
    "df_estats = df_clubes.copy()\n",
    "\n",
    "janelas = [5, 38]\n",
    "\n",
    "for janela in janelas:\n",
    "    \n",
    "    if janela == 5:\n",
    "        \n",
    "        df_estats[f'sum_odd_nas_ultimas_{janela}_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['odd_vitoria']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = janela).sum())\n",
    "        )\n",
    "\n",
    "        df_estats[f'median_odd_nas_ultimas_{janela}_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['odd_vitoria']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = janela).median())\n",
    "        )\n",
    "\n",
    "        df_estats[f'mean_odd_nas_ultimas_{janela}_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['odd_vitoria']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = janela).mean())\n",
    "        )\n",
    "\n",
    "        df_estats[f'std_odd_nas_ultimas_{janela}_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['odd_vitoria']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = janela).std())\n",
    "        )\n",
    "\n",
    "        df_estats[f'max_odd_nas_ultimas_{janela}_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['odd_vitoria']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = janela).max())\n",
    "        )\n",
    "\n",
    "        df_estats[f'min_odd_nas_ultimas_{janela}_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['odd_vitoria']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = janela).min())\n",
    "        )\n",
    "\n",
    "        df_estats[f'prop_jogos_como_favorito_nas_ultimas_{janela}_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['flag_favorito']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = janela).mean())\n",
    "        )\n",
    "\n",
    "        df_estats[f'prop_jogos_vitoriosos_nas_ultimas_{janela}_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['flag_vitoria']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = janela).mean())\n",
    "        )\n",
    "\n",
    "        df_estats[f'delta_odd_nas_ultimas_{janela}_partidas'] = (\n",
    "            df_estats[f'max_odd_nas_ultimas_{janela}_partidas'] - df_estats[f'min_odd_nas_ultimas_{janela}_partidas']\n",
    "        )\n",
    "\n",
    "        df_estats[f'cv_odd_nas_ultimas_{janela}_partidas'] = (\n",
    "            df_estats[f'std_odd_nas_ultimas_{janela}_partidas']/df_estats[f'mean_odd_nas_ultimas_{janela}_partidas']\n",
    "        )\n",
    "        \n",
    "    elif janela == 38:\n",
    "\n",
    "        df_estats[f'sum_odd_nas_ultimas_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['odd_vitoria']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = 1).sum())\n",
    "        )\n",
    "\n",
    "        df_estats[f'median_odd_nas_ultimas_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['odd_vitoria']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = 1).median())\n",
    "        )\n",
    "\n",
    "        df_estats[f'mean_odd_nas_ultimas_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['odd_vitoria']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = 1).mean())\n",
    "        )\n",
    "\n",
    "        df_estats[f'std_odd_nas_ultimas_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['odd_vitoria']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = 1).std())\n",
    "        )\n",
    "\n",
    "        df_estats[f'max_odd_nas_ultimas_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['odd_vitoria']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = 1).max())\n",
    "        )\n",
    "\n",
    "        df_estats[f'min_odd_nas_ultimas_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['odd_vitoria']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = 1).min())\n",
    "        )\n",
    "\n",
    "        df_estats[f'prop_jogos_como_favorito_nas_ultimas_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['flag_favorito']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = 1).mean())\n",
    "        )\n",
    "\n",
    "        df_estats[f'prop_jogos_vitoriosos_nas_ultimas_partidas'] = (\n",
    "            df_estats\n",
    "            .sort_values(by = 'data')\n",
    "            .groupby(['clube','temporada'])['flag_vitoria']\n",
    "            .transform(lambda x: x.shift(1).rolling(window = janela, min_periods = 1).mean())\n",
    "        )\n",
    "        \n",
    "        df_estats[f'delta_odd_nas_ultimas_partidas'] = (\n",
    "            df_estats[f'max_odd_nas_ultimas_partidas'] - df_estats[f'min_odd_nas_ultimas_partidas']\n",
    "        )\n",
    "\n",
    "        df_estats[f'cv_odd_nas_ultimas_partidas'] = (\n",
    "            df_estats[f'std_odd_nas_ultimas_partidas']/df_estats[f'mean_odd_nas_ultimas_partidas']\n",
    "        )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cols = ['clube','temporada','data']\n",
    "\n",
    "target = ['Target']\n",
    "\n",
    "public = ['is_test']\n",
    "\n",
    "features = [x for x in df_estats.columns if 'partidas' in x]\n",
    "\n",
    "mandantes = df_estats.loc[df_estats['mando'] == 1, base_cols+features]\n",
    "visitantes = df_estats.loc[df_estats['mando'] == 0, base_cols+features]\n",
    "\n",
    "mandantes = mandantes.rename(columns={'clube': 'Home', 'temporada': 'Season', 'data': 'Date'})\n",
    "visitantes = visitantes.rename(columns={'clube': 'Away', 'temporada': 'Season', 'data': 'Date'})\n",
    "\n",
    "relational_mandante = ['Season', 'Date', 'Home']\n",
    "relational_visitante = ['Season', 'Date', 'Away']\n",
    "\n",
    "filter = ['Season', 'Date', 'Home', 'Away', 'Res']\n",
    "\n",
    "merge_origin_mandante = pd.merge(df[filter + target + public], mandantes, on = relational_mandante, how = 'left')\n",
    "\n",
    "renamed_estat_features_mandante = [coluna + '_mandante' for coluna in features]\n",
    "\n",
    "merge_origin_mandante.rename(columns=dict(zip(features, renamed_estat_features_mandante)), inplace=True)\n",
    "\n",
    "merge_origin_visitante = pd.merge(df[filter + target + public], visitantes, on = relational_visitante, how = 'left')\n",
    "\n",
    "renamed_estat_features_visitante = [coluna + '_visitante' for coluna in features]\n",
    "\n",
    "merge_origin_visitante.rename(columns=dict(zip(features, renamed_estat_features_visitante)), inplace=True)\n",
    "\n",
    "df_features = merge_origin_mandante.merge(merge_origin_visitante, on = filter + target + public, how = 'left')\n",
    "\n",
    "for feature in features:\n",
    "    if 'jogos' not in feature:\n",
    "        df_features[f'ratio_{feature}'] = df_features[f'{feature}_mandante'] / df_features[f'{feature}_visitante']\n",
    "        df_features[f'diff_{feature}'] = df_features[f'{feature}_mandante'] - df_features[f'{feature}_visitante']\n",
    "    else:\n",
    "        df_features[f'diff_{feature}'] = df_features[f'{feature}_mandante'] - df_features[f'{feature}_visitante']\n",
    "\n",
    "df_features = df_features.drop(columns = renamed_estat_features_mandante + renamed_estat_features_visitante).dropna()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(df_features.shape)\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegLogOptimizator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        x,\n",
    "        y,\n",
    "        target,\n",
    "        features,\n",
    "        categorical_features=[],\n",
    "        niter=10,\n",
    "        metric_eval=\"AUC\",\n",
    "        metric_method=\"default\",\n",
    "        thr=0.5,\n",
    "        col_safra=None,\n",
    "        early_stopping_rounds=3,\n",
    "        eval_n_features=False,\n",
    "        filename_storage=\"reg_log_search\",\n",
    "        save_in_txt=True,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        metric_eval: 'AUC' or 'KS'\n",
    "        metric_method: 'default', 'min', 'range'\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.features = features\n",
    "        self.categorical_features = categorical_features\n",
    "        self.target = target\n",
    "        self.col_safra = col_safra\n",
    "        self.niter = niter\n",
    "        self.metric_eval = metric_eval\n",
    "        self.metric_method = metric_method\n",
    "        self.thr = thr\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.eval_n_features = eval_n_features\n",
    "        self.verbose = verbose\n",
    "        self.best_auc = 0.0\n",
    "        self.score = 0.0\n",
    "        self.iterations_not_improving = 0\n",
    "        self.iterations = 0\n",
    "        self.filename_storage = filename_storage\n",
    "        self.save_in_txt = save_in_txt\n",
    "\n",
    "    def get_optimal_params(self):\n",
    "        if self.save_in_txt:\n",
    "            self.create_log()\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        objective = self.generate_objective_function(\n",
    "            self.x,\n",
    "            self.y,\n",
    "            self.target,\n",
    "            self.features,\n",
    "            self.categorical_features,\n",
    "        )\n",
    "        storage = JournalStorage(JournalFileStorage(f\"{self.filename_storage}.log\"))\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            study_name=\"Hyperparameter search\",\n",
    "            storage=storage,\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "        study.optimize(\n",
    "            objective,\n",
    "            n_trials=self.niter,\n",
    "            callbacks=[self.early_stopping_fn],\n",
    "            n_jobs=1,\n",
    "        )\n",
    "        best_trial = study.best_trial\n",
    "        return best_trial, study\n",
    "\n",
    "    def early_stopping_fn(\n",
    "        self, study: optuna.study.Study, trial: optuna.trial.FrozenTrial\n",
    "    ):\n",
    "        if self.iterations_not_improving >= self.early_stopping_rounds:\n",
    "            study.stop()\n",
    "\n",
    "    def update_best_params(self, score, test_metrics):\n",
    "        self.iterations += 1\n",
    "        if score > self.best_auc:\n",
    "            self.iterations_not_improving = 0\n",
    "            self.best_auc = score\n",
    "            if self.verbose:\n",
    "                console.print(\n",
    "                    f\"|Iteration {self.iterations}| New parameters found - {self.metric_eval} of {np.mean(test_metrics):.4f} ({self.best_auc:.4f})\"\n",
    "                )\n",
    "        else:\n",
    "            self.iterations_not_improving += 1\n",
    "\n",
    "    def ks(self, y, y_pred):\n",
    "        return ks_2samp(y_pred[y == 1], y_pred[y != 1]).statistic\n",
    "\n",
    "    def predict(self, model, x):\n",
    "        return model.predict_proba(x)[:, 1]\n",
    "\n",
    "    def get_metric_min(self, df, target, metric_eval, col_safra):\n",
    "        if metric_eval == \"KS\":\n",
    "            score_min = (\n",
    "                df.groupby(col_safra)\n",
    "                .apply(lambda x: self.ks(x[target], x[\"prob\"]) * 100)\n",
    "                .min()\n",
    "            )\n",
    "        else:\n",
    "            score_min = (\n",
    "                df.groupby(col_safra)\n",
    "                .apply(lambda x: metrics.roc_auc_score(x[target], x[\"prob\"]))\n",
    "                .min()\n",
    "            )\n",
    "        return score_min\n",
    "\n",
    "    def get_metric_range(self, df, target, metric_eval, col_safra):\n",
    "        if metric_eval == \"KS\":\n",
    "            score_min = (\n",
    "                df.groupby(col_safra)\n",
    "                .apply(lambda x: self.ks(x[target], x[\"prob\"]) * 100)\n",
    "                .min()\n",
    "            )\n",
    "            score_max = (\n",
    "                df.groupby(col_safra)\n",
    "                .apply(lambda x: self.ks(x[target], x[\"prob\"]) * 100)\n",
    "                .max()\n",
    "            )\n",
    "        else:\n",
    "            score_min = (\n",
    "                df.groupby(col_safra)\n",
    "                .apply(lambda x: metrics.roc_auc_score(x[target], x[\"prob\"]))\n",
    "                .min()\n",
    "            )\n",
    "            score_max = (\n",
    "                df.groupby(col_safra)\n",
    "                .apply(lambda x: metrics.roc_auc_score(x[target], x[\"prob\"]))\n",
    "                .max()\n",
    "            )\n",
    "        range_ = score_max - score_min\n",
    "        return range_\n",
    "\n",
    "    def get_metric(self, df, target, metric_eval):\n",
    "        if metric_eval == \"KS\":\n",
    "            score = self.ks(df[target], df[\"prob\"]) * 100\n",
    "        else:\n",
    "            score = metrics.roc_auc_score(df[target], df[\"prob\"])\n",
    "        return score\n",
    "\n",
    "    def decision(self, metric_train, metric_test, metric_otm, thr=5):\n",
    "        return 0 if np.abs(metric_train - metric_test) > thr else metric_otm\n",
    "\n",
    "    def generate_objective_function(self, x, y, target, features, categorical_features):\n",
    "        def objective(\n",
    "            trial,\n",
    "            x=x,\n",
    "            y=y,\n",
    "            target=target,\n",
    "            features=features,\n",
    "            categorical_features=categorical_features,\n",
    "        ):\n",
    "            numerical_features = [x for x in features if x not in categorical_features]\n",
    "\n",
    "            parameters = {\n",
    "                \"penalty\": trial.suggest_categorical(\n",
    "                    \"penalty\", [\"l1\", \"l2\", \"elasticnet\"]\n",
    "                ),\n",
    "                \"C\": trial.suggest_loguniform(\"C\", 0.001, 10),\n",
    "                \"class_weight\": trial.suggest_categorical(\n",
    "                    \"class_weight\", [\"balanced\"]\n",
    "                ),\n",
    "                \"max_iter\": trial.suggest_int(\"max_iter\", 100, 1000, 50),\n",
    "            }\n",
    "\n",
    "            if parameters[\"penalty\"] == \"l1\":\n",
    "                parameters[\"solver\"] = trial.suggest_categorical(\n",
    "                    \"solver_l1\", [\"liblinear\", \"saga\"]\n",
    "                )\n",
    "            elif parameters[\"penalty\"] == \"l2\":\n",
    "                parameters[\"solver\"] = trial.suggest_categorical(\n",
    "                    \"solver_l2\",\n",
    "                    [\n",
    "                        \"lbfgs\",\n",
    "                        \"liblinear\",\n",
    "                        \"newton-cg\",\n",
    "                        \"newton-cholesky\",\n",
    "                        \"sag\",\n",
    "                        \"saga\",\n",
    "                    ],\n",
    "                )\n",
    "            elif parameters[\"penalty\"] == \"elasticnet\":\n",
    "                parameters[\"solver\"] = trial.suggest_categorical(\n",
    "                    \"solver_elasticnet\", [\"saga\"]\n",
    "                )\n",
    "                parameters[\"l1_ratio\"] = trial.suggest_float(\n",
    "                    name=\"l1_ratio\", low=0.0, high=1.0, step=0.05\n",
    "                )\n",
    "            else:\n",
    "                parameters[\"solver\"] = trial.suggest_categorical(\n",
    "                    \"solver_none\",\n",
    "                    [\n",
    "                        \"lbfgs\",\n",
    "                        \"liblinear\",\n",
    "                        \"newton-cg\",\n",
    "                        \"newton-cholesky\",\n",
    "                        \"sag\",\n",
    "                        \"saga\",\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "\n",
    "            # Transformer das categóricas\n",
    "            encoder = trial.suggest_categorical(\n",
    "                \"encoder_categorical\", \n",
    "                [\n",
    "                    \"woe\", \n",
    "                    \"onehot\", \n",
    "                    \"ordenc\"\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if encoder == \"woe\":\n",
    "                encoder_sel = WOEEncoder()\n",
    "            if encoder == \"onehot\":\n",
    "                encoder_sel = OneHotEncoder()\n",
    "            if encoder == \"ordenc\":\n",
    "                encoder_sel = OrdinalEncoder()\n",
    "\n",
    "            cat_transformer = Pipeline(steps=[(\"encoder\", encoder_sel)])\n",
    "\n",
    "            # Transformer das numéricas\n",
    "            imputer = trial.suggest_categorical(\n",
    "                \"imputer_missing\",\n",
    "                [\n",
    "                    \"simple_med\",\n",
    "                ]  \n",
    "            )\n",
    "\n",
    "            scaler = trial.suggest_categorical(\n",
    "                \"scaler_val\",\n",
    "                [\n",
    "                    \"standard\",\n",
    "                    \"robust\"\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if imputer == \"simple_med\":\n",
    "                imputer_sel = SimpleImputer(strategy=\"median\")\n",
    "            if imputer == \"simple_cte\":\n",
    "                imputer_sel = SimpleImputer(strategy=\"constant\", fill_value=-999)\n",
    "            if scaler == \"standard\":\n",
    "                scaler_sel = StandardScaler()\n",
    "            if scaler == \"robust\":\n",
    "                scaler_sel = RobustScaler()\n",
    "\n",
    "            num_transformer = Pipeline(steps=[(\"imputer\", imputer_sel), (\"scaler\", scaler_sel)])\n",
    "\n",
    "            # Compondo os pré-processadores\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"num\", num_transformer, numerical_features),\n",
    "                    (\"cat\", cat_transformer, categorical_features),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Definição do modelo\n",
    "            model = LogisticRegression(**parameters)\n",
    "\n",
    "            pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"RegLog\", model)])\n",
    "            \n",
    "            if self.eval_n_features:\n",
    "                \n",
    "                pipeline.fit(x[features], y[[target]])\n",
    "                \n",
    "                coef = pipeline.steps[-1][1].coef_[0]\n",
    "                feature_importance = pd.DataFrame(\n",
    "                    {\"var\": features, \"Importance\": np.abs(coef)}\n",
    "                )\n",
    "                feature_importance = feature_importance.sort_values(\n",
    "                    \"Importance\", ascending=False\n",
    "                ).reset_index(drop=True)\n",
    "                \n",
    "                features_fi = list(feature_importance[\"var\"])\n",
    "                \n",
    "                for i in range(len(features_fi)):\n",
    "                    for k in range(len(features)):\n",
    "                        if features[k] in features_fi[i]:\n",
    "                            features_fi[i] = features[k]\n",
    "                \n",
    "                features_fi = list(dict.fromkeys(features_fi))\n",
    "                \n",
    "                n_features = trial.suggest_int(\"n_features\", 1, len(features))\n",
    "                \n",
    "                selected_features = features_fi[:n_features]\n",
    "                \n",
    "                trial.set_user_attr(\"selected_features\", selected_features)\n",
    "                \n",
    "                features = [x for x in selected_features]\n",
    "                \n",
    "                categorical_features = [x for x in self.categorical_features if x in selected_features]\n",
    "                \n",
    "                preprocessor = ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        (\n",
    "                            \"num\",\n",
    "                            num_transformer,\n",
    "                            [x for x in features if x not in categorical_features],\n",
    "                        ),\n",
    "                        (\"cat\", cat_transformer, categorical_features),\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"RegLog\", model)])\n",
    "\n",
    "            train_metrics = np.zeros(5)\n",
    "            test_metrics = np.zeros(5)\n",
    "\n",
    "            kf = StratifiedKFold(shuffle=True, random_state=42)\n",
    "\n",
    "            for i, (idx_train, idx_test) in enumerate(kf.split(x, y)):\n",
    "                x_train, y_train = x.iloc[idx_train], y.iloc[idx_train]\n",
    "                x_test, y_test = x.iloc[idx_test], y.iloc[idx_test]\n",
    "\n",
    "                pipeline.fit(x_train, y_train)\n",
    "                y_pred_train = self.predict(pipeline, x_train)\n",
    "                y_pred_test = self.predict(pipeline, x_test)\n",
    "                y_train[\"prob\"] = y_pred_train\n",
    "                y_test[\"prob\"] = y_pred_test\n",
    "\n",
    "                if self.metric_method == \"min\":\n",
    "                    train_metrics[i] = self.get_metric_min(\n",
    "                        y_train, target, self.metric_eval, self.col_safra\n",
    "                    )\n",
    "                    test_metrics[i] = self.get_metric_min(\n",
    "                        y_test, target, self.metric_eval, self.col_safra\n",
    "                    )\n",
    "                elif self.metric_method == \"range\":\n",
    "                    train_metrics[i] = self.get_metric_range(\n",
    "                        y_train, target, self.metric_eval, self.col_safra\n",
    "                    )\n",
    "                    test_metrics[i] = self.get_metric_range(\n",
    "                        y_test, target, self.metric_eval, self.col_safra\n",
    "                    )\n",
    "                else:\n",
    "                    train_metrics[i] = self.get_metric(\n",
    "                        y_train, target, self.metric_eval\n",
    "                    )\n",
    "                    test_metrics[i] = self.get_metric(\n",
    "                        y_test, target, self.metric_eval\n",
    "                    )\n",
    "\n",
    "            metrics = {\n",
    "                \"train_metrics\": list(train_metrics),\n",
    "                \"test_metrics\": list(test_metrics),\n",
    "                \"train_metric\": np.mean(train_metrics),\n",
    "                \"test_metric\": np.mean(test_metrics),\n",
    "            }\n",
    "\n",
    "            for key in metrics:\n",
    "                trial.set_user_attr(key, metrics[key])\n",
    "\n",
    "            metric_otm = np.mean(test_metrics) - np.std(test_metrics)\n",
    "            value = self.decision(\n",
    "                np.mean(train_metrics), np.mean(test_metrics), metric_otm, thr=self.thr\n",
    "            )\n",
    "            self.update_best_params(value, test_metrics)\n",
    "            if self.save_in_txt:\n",
    "                f = open(f\"{self.log_file}.txt\", \"a\")\n",
    "                f.write(\n",
    "                    f\"{self.iterations};{parameters};{list(metrics['train_metrics'])};{list(metrics['test_metrics'])}\\n\"\n",
    "                )\n",
    "                f.close()\n",
    "            return value\n",
    "\n",
    "        return objective\n",
    "\n",
    "    def create_log(self):\n",
    "        time_ref = datetime.datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "        self.log_file = f\"resume_reg_log_opt_{time_ref}\"\n",
    "        f = open(f\"{self.log_file}.txt\", \"w\")\n",
    "        f = open(f\"{self.log_file}.txt\", \"a\")\n",
    "        f.write(f\"iter;parameters;train_metrics;test_metrics\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_directory()\n",
    "\n",
    "features = list(df_features.drop(columns = ['Season','Date','Home','Away', 'Res', 'Target','is_test']).columns)\n",
    "target = ['Target']\n",
    "\n",
    "model_opt = RegLogOptimizator (\n",
    "\n",
    "            x = df_features.loc[df_features['is_test'] == 0,features],\n",
    "            y = df_features.loc[df_features['is_test'] == 0,target],\n",
    "            target = 'Target',\n",
    "            features = features,\n",
    "            categorical_features = [],\n",
    "            niter = 1000,\n",
    "            metric_eval = \"AUC\",\n",
    "            metric_method = \"default\",\n",
    "            thr = 0.05, \n",
    "            col_safra = None,\n",
    "            early_stopping_rounds = 200,\n",
    "            eval_n_features = True,\n",
    "            filename_storage = \"1st_run_experiment_1\",\n",
    "            save_in_txt = True\n",
    "\n",
    "        )\n",
    "    \n",
    "best_trial, study = model_opt.get_optimal_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_features.loc[df_features['is_test'] == 0, features], df_features.loc[df_features['is_test'] == 0, target]\n",
    "X_test, y_test = df_features.loc[df_features['is_test'] == 1, features], df_features.loc[df_features['is_test'] == 1, target]\n",
    "\n",
    "df_results = {\n",
    "\n",
    "    'id': [],\n",
    "    'params': [],\n",
    "    'qtd_features': [],\n",
    "    'features': [],\n",
    "    'auc_train_cv': [],\n",
    "    'auc_test_cv': [],\n",
    "    'auc_train': [],\n",
    "    'auc_test': [],\n",
    "\n",
    "}\n",
    "\n",
    "for j, study_trial in enumerate(study.trials):\n",
    "\n",
    "    if study_trial.state != TrialState.COMPLETE or study_trial.value == 0:\n",
    "        continue  # Skip the trials that did not complete successfully\n",
    "\n",
    "    model_params = {k: v for k, v in study_trial.params.items() if k not in ['encoder_categorical','imputer_missing','scaler_val','n_features']}\n",
    "    solver_key = next((key for key in model_params if 'solver' in key), None)\n",
    "    if solver_key:\n",
    "        model_params['solver'] = model_params.pop(solver_key)\n",
    "\n",
    "    num_features = study_trial.params['n_features']\n",
    "    model_features = study_trial.user_attrs['selected_features']\n",
    "\n",
    "    scaler_dict = {k: v for k, v in study_trial.params.items() if k == 'scaler_val'}\n",
    "    \n",
    "    if scaler_dict['scaler_val'] == 'standard':\n",
    "        transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "    elif scaler_dict['scaler_val'] == 'robust':\n",
    "        transformer = Pipeline(steps=[(\"scaler\", RobustScaler())])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"features\", transformer, model_features),\n",
    "                ]\n",
    "            )\n",
    "    \n",
    "    clf = LogisticRegression(**model_params)\n",
    "    pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"RegLog\", clf)])\n",
    "\n",
    "    pipeline.fit(X_train[model_features], y_train)\n",
    "    y_prob_train = pipeline.predict_proba(X_train[model_features])[:, 1]\n",
    "    auc_train = metrics.roc_auc_score(y_train, y_prob_train)\n",
    "\n",
    "    y_prob_test = pipeline.predict_proba(X_test[model_features])[:, 1]\n",
    "    auc_test = metrics.roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "    for i, k in enumerate(df_results.keys()): df_results[k].append([f\"{j:03d}\", \n",
    "                                                                    model_params, \n",
    "                                                                    num_features, \n",
    "                                                                    model_features, \n",
    "                                                                    round(study_trial.user_attrs['train_metric'] * 100, 2),\n",
    "                                                                    round(study_trial.user_attrs['test_metric'] * 100, 2),\n",
    "                                                                    round(auc_train * 100, 2), \n",
    "                                                                    round(auc_test * 100, 2)][i])\n",
    "\n",
    "df_results = pd.DataFrame(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values(by = 'auc_test', ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.loc[df_results['id'] == '555', 'features'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "params = deepcopy(best_trial.params)\n",
    "params['random_state'] = 42\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'penalty': params['penalty'],\n",
    " 'C': params['C'],\n",
    " 'class_weight': params['class_weight'],\n",
    " 'max_iter': params['max_iter'],\n",
    " 'solver': params[f'solver_{params[\"penalty\"]}'],\n",
    " 'random_state': params['random_state']}\n",
    "\n",
    "#transformer = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", RobustScaler())])\n",
    "transformer = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "\n",
    "sel_features = best_trial.user_attrs['selected_features']\n",
    "\n",
    "# preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"features\", transformer, sel_features),\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and Prediction\n",
    "X_train, y_train = df_features.loc[df_features['is_test'] == 0, sel_features], df_features.loc[df_features['is_test'] == 0, target]\n",
    "X_test, y_test = df_features.loc[df_features['is_test'] == 1, sel_features], df_features.loc[df_features['is_test'] == 1, target]\n",
    "\n",
    "clf = LogisticRegression(**model_params)\n",
    "\n",
    "pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"RegLog\", clf)])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_prob_train = clf.predict_proba(X_train)[:, 1]\n",
    "auc_train = metrics.roc_auc_score(y_train, y_prob_train)\n",
    "\n",
    "y_prob_test = clf.predict_proba(X_test)[:, 1]\n",
    "auc_test = metrics.roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "print(\"Auc de treino:\", round(auc_train * 100, 2))\n",
    "print(\"Auc de teste: \", round(auc_test * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients from the logistic regression model\n",
    "coefficients = pipeline.named_steps['RegLog'].coef_[0]\n",
    "\n",
    "# Create a DataFrame of feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'var': sel_features,\n",
    "    'importance': np.abs(coefficients)\n",
    "})\n",
    "\n",
    "importance_df = importance_df.sort_values(by = 'importance', ascending = False)\n",
    "\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"model_experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"1st-run-experiment-model-study\", description=\"Model to predict home win in Brazilian Soccer Championship of first division using the informations of the last 5 matches\"):\n",
    "    \n",
    "    mlflow.log_params(model_params)\n",
    "\n",
    "    mlflow.log_param(\"features\", features)\n",
    "\n",
    "    mlflow.log_metric(\"train_auc\", auc_train)\n",
    "\n",
    "    mlflow.log_metric(\"test_auc\", auc_test)\n",
    "\n",
    "    mlflow.sklearn.log_model(pipeline, \"classifier\")\n",
    "    \n",
    "    plot_metrics(df = df_features[df_features['is_test'] == 1], clf = clf, features = features, run = 1, experiment = 'model_experiment', mlflow_path=\"classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp-tracking-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
